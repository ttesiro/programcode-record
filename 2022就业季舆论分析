import jieba
import wordcloud
#加载情感分析模块
from snownlp import SnowNLP
from snownlp import sentiment
import pandas as pd
import matplotlib.pyplot as plt
import re
import collections
import jieba

from pyecharts.charts import WordCloud
from pyecharts import options as opts

#导入样例数据
aa ='/Users/bytedance/Downloads/评论.xlsx'
#读取文本数据
df=pd.read_excel(aa)
print(df.head())

# 拼接所有岗位类别为数据科学的岗位描述
string_data = ''
for i in df['评论内容']:
    string_data += str(i)

# 2.文本预处理，去除各种标点符号，不然统计词频时会统计进去
# 定义正则表达式匹配模式，其中的|代表或
pattern = re.compile(u'\t|\n| |；|\.|。|：|：\.|-|:|\d|;|、|，|\)|\(|\?|"')
# 将符合模式的字符去除，re.sub代表替换，把符合pattern的替换为空
string_data = re.sub(pattern, '', string_data)

# 3.文本分词
seg_list_exact = jieba.cut(string_data, cut_all=False)  # 精确模式分词
# object_list  = list(seg_list_exact) # list()函数可以把可迭代对象转为列表

# 4.运用过滤词表优化掉常用词，比如“的”这些词，不然统计词频时会统计进去
object_list = []

# 读取过滤词表
with open('/Users/bytedance/Downloads/rremove_words.txt', 'r', encoding="utf-8") as fp:
    remove_words = fp.read().split()

# 循环读出每个分词
for word in seg_list_exact:
    #看每个分词是否在常用词表中或结果是否为空或\xa0不间断空白符，如果不是再追加
    if word not in remove_words and word != ' ' and word != '\xa0':
        object_list.append(word)  # 分词追加到列表

# 5.进行词频统计，使用pyecharts生成词云
# 词频统计
word_counts = collections.Counter(object_list)  # 对分词做词频统计
word_counts_top = word_counts.most_common(100)  # 获取前100最高频的词

# 绘图
# https://gallery.pyecharts.org/#/WordCloud/wordcloud_custom_mask_image
# 去pyecharts官网找模板代码复制出来修改
c = (
    WordCloud()
    .add("", word_counts_top)#根据词频最高的词
    .render("/Users/bytedance/Downloads/wordcloud2.html")#生成页面
)
